{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Preprocessing</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is related to the prepocessing of the requirements field of the Fake_Real_Job_Posting.csv file. More especially, we will focus on all rows were the requirements field is not missing (not \"Not Mentioned\"). This field is thus considered to non-informative when there is some missing values and it can create noises in the data. It could be then preferable not to select them.  \n",
    "The preprocessing steps that will be implemented are the following:\n",
    "\n",
    "- <u>Removal of URLs:</u>  \n",
    "Removing URLs is important to ensure that web links do not affect the analysis of text. URLs typically do not contribute to the content or semantics of text, so their removal helps maintain focus on the meaningful information.   \n",
    "\n",
    "- <u>Tokenization:</u>  \n",
    "We will start with the tokenization in order to divide the text in token (or words). By starting to tokenize the text, it will then be easier to implement the other preprocessing steps (the data structure will be more manipulable).\n",
    "\n",
    "- <u>Lower casing:</u>  \n",
    "Then we will do the lower casing step which is important for text standardization. It could also be helpful if we do some text feature exraction as it helps to combine the same words together and thus reducing the duplications.\n",
    "\n",
    "- <u>Removal of ponctuation:</u>  \n",
    "Eliminating punctuation marks from the text is crucial to get rid of unnecessary token and to make sure they don't cause problems in the following processing steps. Punctuation marks usually don't hold significant meaning so they can be safely removed.\n",
    "\n",
    "\n",
    "- <u>Removal of stopwords:</u>  \n",
    "The removal of stopwords is crucial for text data preprocessing. Stopwords are common words that can be removed since they add little semantic value, they don't add valuable information for the text analysis. This step reduces noise and the dimensionality of the data.  \n",
    "\n",
    "- <u>Lemmatization:</u>  \n",
    "Finally, we will perform lemmatization to ensure that words are reduced to their base or dictionary form, which helps in standardizing the text.\n",
    "\n",
    "The order of the preprocessing steps is important. Indeed, after removing the URLs, the tokenization serves as the foundation for all other steps, ensuring that text is divided into meaningful units (words, tokens). Lowercasing is performed next to standardize text and reduce duplications. Then, punctuation removal, stopword removal and lemmatization help clean the text, reduce its dimension and thus reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/0tzd3xtn06nd92x1lf625rv80000gn/T/ipykernel_61942/1691018282.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_reduced['requirements'] = data_reduced['requirements'].astype(str).apply(preprocessor.preprocessing)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "path = 'Data/Fake_Real_Job_Posting.csv'\n",
    "data_full = pd.read_csv(path)\n",
    "\n",
    "data_reduced = data_full[data_full['requirements'] != \"Not Mentioned\"]\n",
    "\n",
    "class PreprocessingClass:\n",
    "    def remove_URL(self, text):\n",
    "        url_pattern = r'#URL_[a-f0-9]+#'\n",
    "        return re.sub(url_pattern, '', text)\n",
    "\n",
    "    def preprocessing(self, text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # URLs removal\n",
    "        cleaned_text = self.remove_URL(text)\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(cleaned_text)\n",
    "        # Lowercasing + remove non number and letter character (ponctuation removal)\n",
    "        words = [word.lower() for word in tokens if word.isalnum()]\n",
    "        # Stopwords removal\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        # Lemmatization\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "        return ' '.join(words)\n",
    "\n",
    "# instantiate the class\n",
    "preprocessor = PreprocessingClass()\n",
    "\n",
    "# apply the preprocessing function to the \"requirements\" field\n",
    "data_reduced['requirements'] = data_reduced['requirements'].astype(str).apply(preprocessor.preprocessing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        year experience ux ui design portfolio contain...\n",
       "1        food graduate similar disciplineadvanced haccp...\n",
       "2        job duty responsibility analysisperform root c...\n",
       "3        international broadcaster shall least five 5 y...\n",
       "4        experience professional environmentsare net na...\n",
       "                               ...                        \n",
       "17875    essential relational database theory understan...\n",
       "17876    need somebody really love adwords google servi...\n",
       "17877    requires ability become forklift able effectiv...\n",
       "17878    education experiencebachelor degree physic mat...\n",
       "17879    fluency englishsimilar professional experience...\n",
       "Name: requirements, Length: 15185, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduced['requirements']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
